2021-03-03 22:41:34 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: Day029)
2021-03-03 22:41:34 [scrapy.utils.log] INFO: Versions: lxml 4.6.2.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1i  8 Dec 2020), cryptography 3.3.1, Platform Windows-10-10.0.19041-SP0
2021-03-03 22:41:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2021-03-03 22:41:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'Day029',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': './output/ptt.log',
 'NEWSPIDER_MODULE': 'Day029.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['Day029.spiders']}
2021-03-03 22:41:34 [scrapy.extensions.telnet] INFO: Telnet Password: 997d07e8b7307736
2021-03-03 22:41:34 [py.warnings] WARNING: c:\users\user\anaconda3\envs\crawler\lib\site-packages\scrapy\extensions\feedexport.py:247: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details
  exporter = cls(crawler)

2021-03-03 22:41:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
2021-03-03 22:41:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2021-03-03 22:41:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2021-03-03 22:41:34 [scrapy.middleware] INFO: Enabled item pipelines:
['Day029.pipelines.Day029Pipeline']
2021-03-03 22:41:34 [scrapy.core.engine] INFO: Spider opened
2021-03-03 22:41:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2021-03-03 22:41:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2021-03-03 22:41:35 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.ptt.cc/robots.txt> (referer: None)
2021-03-03 22:41:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/DataScience/index.html> (referer: None)
2021-03-03 22:41:35 [py.warnings] WARNING: C:\Users\User\Desktop\AI大數據\Jupyterlab\Day029\Day029\spiders\Pttcrawler.py:23: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line 23 of the file C:\Users\User\Desktop\AI大數據\Jupyterlab\Day029\Day029\spiders\Pttcrawler.py. To get rid of this warning, pass the additional argument 'features="lxml"' to the BeautifulSoup constructor.

  soup = BeautifulSoup(response.text)

2021-03-03 22:41:35 [Pttcrawler] DEBUG: Parse article [情報] 新手向kaggle自辦競賽 ptt推噓文預測
2021-03-03 22:41:35 [Pttcrawler] DEBUG: Parse article [問題] 新手openpose與深度學習問題
2021-03-03 22:41:35 [Pttcrawler] DEBUG: Parse article Re: [討論] 分團問題!?
2021-03-03 22:41:35 [Pttcrawler] DEBUG: Parse article [問題] spark 開發求書
2021-03-03 22:41:35 [Pttcrawler] DEBUG: Parse article [問題] Tensorflow data pipeline 問題
2021-03-03 22:41:35 [Pttcrawler] DEBUG: Reach the last article
2021-03-03 22:41:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/DataScience/M.1612527945.A.1FE.html> (referer: https://www.ptt.cc/bbs/DataScience/index.html)
2021-03-03 22:41:36 [py.warnings] WARNING: C:\Users\User\Desktop\AI大數據\Jupyterlab\Day029\Day029\spiders\Pttcrawler.py:56: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("lxml"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.

The code that caused this warning is on line 56 of the file C:\Users\User\Desktop\AI大數據\Jupyterlab\Day029\Day029\spiders\Pttcrawler.py. To get rid of this warning, pass the additional argument 'features="lxml"' to the BeautifulSoup constructor.

  soup = BeautifulSoup(response.text)

2021-03-03 22:41:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/DataScience/M.1612527945.A.1FE.html>
{'article_author': 'jack1218 (赤城我老婆)',
 'article_content': '\n'
                    '\n'
                    '如題\n'
                    '最近在學NLP 所以爬了ptt八卦版的問卦\n'
                    '準備做推噓文的預測\n'
                    '不過自己玩有點無聊 所以把dataset整理好放到kaggle上 開個小競賽\n'
                    '有興趣的人可以一起玩玩看\n'
                    '期限是一個月\n'
                    '\n'
                    'https://www.kaggle.com/c/ptt-gossiping-push-down-predict/',
 'article_date': 'Fri Feb  5 20:25:43 2021',
 'article_id': 'M.1612527945.A.1FE',
 'article_title': '[情報] 新手向kaggle自辦競賽 ptt推噓文預測',
 'ip': '223.137.94.234',
 'message_count': 29,
 'messages': ' 推推=====分隔線===== 有趣....=====分隔線===== XD=====分隔線===== have '
             'fun!=====分隔線===== 還在念博士時有做過用PTT文章預測投票票數的，當時結果很=====分隔線===== '
             '神奇地好。不過預測推噓文，直覺上要好會需要用手段補充外=====分隔線===== 在背景知識=====分隔線===== '
             '總之這題目確實挺有趣的=====分隔線===== '
             '另外，其實我認為發文作者是其中一個鑑別力很強的feature=====分隔線===== '
             '，這邊沒有提供XD=====分隔線===== 顏色正確就推爆=====分隔線===== 蠻有趣的=====分隔線===== '
             '光從發文作者就可以判斷的case應該要被當outliers吧=====分隔線===== '
             '提供作者有個重點是可以查詢上站次數跟文章次數，而這兩項=====分隔線===== 我認為是很重要的特徵=====分隔線===== '
             '舉一個已經有實用經驗的例子，Youtube的自動判斷機制在接=====分隔線===== '
             '到影片舉報時，他們研究發現最有辨別能力的特徵就是發該影=====分隔線===== '
             '片的使用者是否是很新的帳號，若是則大機率真的是問題影片=====分隔線===== 而針對PTT，我個人的觀察是去看 '
             '文章篇數/上站次數 這個比=====分隔線===== 值，對於文章是不是廢文的機率也有高辨識度=====分隔線===== '
             '若這個比值達到1以上，越高就越可能是廢文=====分隔線===== '
             '但如果比值大約在0.5前後，則相對用心發文的機率較高=====分隔線===== '
             '比值若很接近0（也就是發文很少上站很多），又會反過來變=====分隔線===== '
             '成內容不足的機率提升，但狀況相對比值高於1的輕微=====分隔線===== '
             '另外取得作者還有一項判斷依據，就是有些人可能在特定版面=====分隔線===== '
             '容易被噓但在別的特定版面容易被推，先不提所謂政治傾向，=====分隔線===== '
             '那種在棒球版是大師但在遊戲版是廢文王的情況=====分隔線===== '
             '也是很常見，所以使用者名稱搭配文章發表版面會是一組可能=====分隔線===== 不錯的特徵',
 'url': 'https://www.ptt.cc/bbs/DataScience/M.1612527945.A.1FE.html'}
2021-03-03 22:41:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/DataScience/M.1614079009.A.215.html> (referer: https://www.ptt.cc/bbs/DataScience/index.html)
2021-03-03 22:41:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/DataScience/M.1613921489.A.E25.html> (referer: https://www.ptt.cc/bbs/DataScience/index.html)
2021-03-03 22:41:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/DataScience/M.1613300638.A.731.html> (referer: https://www.ptt.cc/bbs/DataScience/index.html)
2021-03-03 22:41:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/DataScience/M.1613621707.A.C0C.html> (referer: https://www.ptt.cc/bbs/DataScience/index.html)
2021-03-03 22:41:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/DataScience/M.1614079009.A.215.html>
{'article_author': 'Sheepforpart (Sheep)',
 'article_content': '\n'
                    '作業系統: Google colab/ Mac\n'
                    '\n'
                    '問題類別: DL\n'
                    '\n'
                    '使用工具: Tensorflow 2.x\n'
                    '\n'
                    '問題內容:\n'
                    '\n'
                    '想請教一個實作 Tensorflow data pipeline 的問題，我已經研究了好幾天\n'
                    '但還是沒找到一個好方法\n'
                    '\n'
                    '模型的功能是預測一張圖片上哪些地方比較吸引人眼注意\n'
                    '\n'
                    '但麻煩的是原始資料全都是影片，而且除了影片外，我們還要加入聲音來訓練\n'
                    '\n'
                    '整體流程大約是這樣：\n'
                    '\n'
                    '1. 從影片中抽取幾張 frame\n'
                    '2. 從影片中抽取對應的 audio, 並且做成 spectrogram\n'
                    '3. 上一個步驟中的 audio 也保留 wave\n'
                    '\n'
                    '\n'
                    '我目前查資料看起來 TFRecord 的功能好像很強，只是真的很複雜，不知道怎麼做到\n'
                    '上面的那些轉換\n'
                    '\n'
                    '另外有一個不是很理想的方法是，先把需要的 frames 和 audio 都先抽好存起來\n'
                    '如果是這個方法的話，我就只需要知道 Tensorflow 要如何同時餵進來自兩個來源\n'
                    '的資料，這邊的重點可能是 shuffle 之後次序依然要維持對應\n'
                    '\n'
                    '這個方法不太理想是因為在實驗完之後，真正使用的資料集非常大，沒辦法先抽好\n'
                    '\n'
                    '麻煩大家給我一些建議了，謝謝',
 'article_date': 'Tue Feb 23 19:16:43 2021',
 'article_id': 'M.1614079009.A.215',
 'article_title': '[問題] Tensorflow data pipeline 問題',
 'ip': '109.57.194.138',
 'message_count': 4,
 'messages': ' 設定兩個Input layer, 再用concatenate layer=====分隔線===== '
             '不熟TFRecord沒法給什麼建議，不過應該只要處理好=====分隔線===== '
             '影片frame和audio的對應，spectrogram可以online從=====分隔線===== audio轉換而來',
 'url': 'https://www.ptt.cc/bbs/DataScience/M.1614079009.A.215.html'}
2021-03-03 22:41:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/DataScience/M.1613921489.A.E25.html>
{'article_author': 'loser113 (洨大魯蛇ㄍ)',
 'article_content': '\n'
                    '作業系統:(ex:mac,win10,win8,win7,linux,etc...)\n'
                    '\n'
                    '問題類別:(ex：ML,SVM,RL,DL,RNN,CNN,NLP,BD,Vis,etc...)\n'
                    '\n'
                    '使用工具: spark kafka hbase\n'
                    '\n'
                    '問題內容:\n'
                    '\n'
                    '最近需要開發資料串流\n'
                    '\n'
                    '想求的 spark 相關書籍 最好有寫道 有關 kafka  串流方法\n'
                    '\n'
                    '除了歐萊禮 有沒有其他推薦 希望不要太舊版',
 'article_date': 'Sun Feb 21 23:31:27 2021',
 'article_id': 'M.1613921489.A.E25',
 'article_title': '[問題] spark 開發求書',
 'ip': '...',
 'message_count': 1,
 'messages': ' use flink',
 'url': 'https://www.ptt.cc/bbs/DataScience/M.1613921489.A.E25.html'}
2021-03-03 22:41:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/DataScience/M.1613300638.A.731.html>
{'article_author': 'kuku951 (猴子)',
 'article_content': '\n'
                    '各位板上大大好\n'
                    '\n'
                    '小弟最近在思考一個問題\n'
                    '\n'
                    '廢話不多說直接來\n'
                    '\n'
                    '我要判斷投手投球的球種是什麼\n'
                    '\n'
                    '理應還說投出每種球種時\n'
                    '\n'
                    '人體的骨架Ex: 手臂出手的角度會不同\n'
                    '\n'
                    '然後\n'
                    '\n'
                    '如果資料集影片切割成圖片後用\n'
                    '\n'
                    'openpose生成人體骨架生成圖\n'
                    '\n'
                    '再丟到InceptionV4裡面提取特徵\n'
                    '\n'
                    '最後再送進去神經網路做學習與驗證\n'
                    '\n'
                    '這樣與\n'
                    '\n'
                    '資料集圖片直接丟如InceptionV4提取特徵再送進去神經網路裡面 的差別在哪邊？\n'
                    '\n'
                    '因為我很納悶，假設有經過openpose的比較好，那深度學習模型要如何去學習投手骨架的\n'
                    '變化，還是應該是也是一直給他定義說 這個是什麼球種這樣？！\n'
                    '\n'
                    '\n',
 'article_date': 'Sun Feb 14 19:03:56 2021',
 'article_id': 'M.1613300638.A.731',
 'article_title': '[問題] 新手openpose與深度學習問題',
 'ip': '27.247.197.86',
 'message_count': 22,
 'messages': ' 就差在一個NN可以直接看到骨架噪音較少啊=====分隔線===== '
             '這裡的骨架變化相當於一種球種特徵，原來NN要自己找重點，=====分隔線===== '
             '妳現在幫他找好了，剩下該怎做，就怎做啊=====分隔線===== '
             '不好意思，有實證說球種跟手臂角度變化的關係嗎？=====分隔線===== '
             '要做分類要給label。影片截圖之後有上下文關係，incep=====分隔線===== '
             'tionV4可能不是好選擇？=====分隔線===== 如果沒記錯的話，球種跟手指握球的方式比較有關。=====分隔線===== '
             '手臂位置多少會有關係，不過也有投手很刻意想辦法把不同球=====分隔線===== '
             '種練到手臂位置差不多，增加打者分辨的難度=====分隔線===== '
             '事實上就打者的角度，想要更早一點看出球路，就是想盡辦法=====分隔線===== '
             '從手臂、手掌、出手點甚至到準備動作微小習慣差異去判斷=====分隔線===== '
             '而投手也會想辦法掩飾修正這些差異=====分隔線===== '
             '所以就這問題來說，我們應該是一球一球給它正確的球種答案=====分隔線===== '
             '，期待學習出來的模型能夠找得出不同球種間的差異而能分辨=====分隔線===== '
             '我們不應該先行猜想手臂到底跟球種有沒有關聯性，而是讓模=====分隔線===== '
             '型去找找看關聯性存不存在=====分隔線===== '
             '另外，這些關聯性高機率每個投手都不一樣，當然你要嘗試找=====分隔線===== '
             '所有投手會不會也多少有什麼共通性也是可以嘗試，但就背景=====分隔線===== '
             '知識來看可能相對不容易找出有意義結論，先以單一投手個別=====分隔線===== '
             '建立模型可能比較有機會=====分隔線===== openpose的license有禁止體育應用 '
             '小心被告=====分隔線===== 沒實際用過，居然有這事啊XD',
 'url': 'https://www.ptt.cc/bbs/DataScience/M.1613300638.A.731.html'}
2021-03-03 22:41:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.ptt.cc/bbs/DataScience/M.1613621707.A.C0C.html>
{'article_author': 'peter308 (pete)',
 'article_content': '\n'
                    '※ 引述《peter308 (pete)》之銘言：\n'
                    ': 在處理龐大的數據的時候\n'
                    ': 經常會透過分團(Clique, cluster analysis) 來將數據做分類\n'
                    ': 我最近有觀察到這種分團問題都會附帶一個關聯矩陣\n'
                    ': 這個矩陣的長相大致上如下:\n'
                    ': https://i.imgur.com/IgE8Y75.jpg\n'
                    ': 因為我之前修群論的時候\n'
                    ': 對於這樣的圖形常常接觸\n'
                    ': 像有一本量子力學的教科書的封面就是這類型的矩陣\n'
                    ': https://i.imgur.com/I9Aa6cU.jpg\n'
                    ': 通常有這樣的圖形出現就表示系統有某種對稱性!\n'
                    ': 不知道各位在數據科學領域(非我的專長)\n'
                    ': 可有聽過有什麼樣的對稱性的狀況或是討論嗎??\n'
                    ': 感謝!!\n'
                    '\n'
                    '\n'
                    '想再聊聊這個話題\n'
                    '\n'
                    '我覺得分群或是分團問題，本身就是一個大哉問。\n'
                    '\n'
                    '通常這類問題 我們都會把哈密頓矩陣在一個比較直觀容易處理的基底ψs上做展開。\n'
                    '\n'
                    '但因為這些ψs所形成的矩陣比較稀疏或是矩陣元素散布各處\n'
                    '\n'
                    '我們通常會做一件事 那就是矩陣對角化或是區塊矩陣對角化\n'
                    '\n'
                    '這件事情的幾何上意義，就是再做一個座標轉換 (similiarity transformation)\n'
                    '\n'
                    '將原本的座標系統轉到一個新的座標系統上\n'
                    '\n'
                    '讓矩陣元素可以變成集中在區塊的對角線上，而非區塊對角線上的元素越小越好。\n'
                    '\n'
                    '\n'
                    '\n'
                    '\n'
                    '這件事情其實在數據科學也常常看見\n'
                    '\n'
                    '像是 PCA, k-means,.....等等\n'
                    '\n'
                    '本質上 和 做block diagonization是非常類似的\n'
                    '\n'
                    '然而這件事情，其實可以從一另一個完全不同方向來著手。\n'
                    '\n'
                    '也就是從觀測系統的對稱性 S 上直接下手。\n'
                    '\n'
                    '================================================================\n'
                    '\n'
                    '比方說 化學的分子點群就是一個很棒的例子\n'
                    '\n'
                    '如果用一個比較簡單的例子 水分子 H20 ( 分子點群為C2v)\n'
                    '\n'
                    '那麼水分子的哈密頓矩陣就可以被區塊對角化\n'
                    '\n'
                    '而區塊對角化後的矩陣所對應的座標系統我們給一個名稱叫做"不可化約的"\n'
                    '\n'
                    "這個區塊對角化後的新的矩陣上的基底ψ'\n"
                    '\n'
                    '會對應C2v的徵值表所能列出的所有不可化約表象, e.g. A1,A2,B1,B2\n'
                    '\n'
                    'http://symmetry.jacobs-university.de/cgi-bin/group.cgi?group=402&option=4\n'
                    '\n'
                    '\n'
                    '================================================================\n'
                    '\n'
                    '回到數據科學上\n'
                    '\n'
                    '針對某個數據資料集\n'
                    '\n'
                    '我們能否在還沒做分群前，就先知道數據資料集的對稱性Ω為何?\n'
                    '\n'
                    "類似前面舉的那個水分子的分子對稱性'C2v'一樣\n"
                    '\n'
                    '事實上，我個人覺得找數據資料集的整體對稱性Ω這件事情\n'
                    '\n'
                    '已經有人再做了\n'
                    '\n'
                    '如果對稱性Ω能事先知道\n'
                    '\n'
                    '那麼理論上我們應該可以加速分群這件事\n'
                    '\n'
                    '因為，在分群前我們其實已經事先知道答案了。\n'
                    '\n'
                    '那就是最後的區塊對角化的矩陣\n'
                    '\n'
                    '一定會在Ω的不可化約表像基底所形成的線性空間上\n'
                    '\n'
                    '這些不可化約表象上的基底，會和這個資料集所具備的整體對稱性Ω有某些關係存在\n'
                    '\n'
                    '如果能知道Ω為何？以及其對應的徵值表和不可化約表象。\n'
                    '\n'
                    '那在對於資料及分群這件事情上\n'
                    '\n'
                    '應該會事半功倍 進而設計出更有快有效率的新的分群演算法!\n'
                    '\n'
                    '\n'
                    '\n'
                    '\n'
                    '\n'
                    '再找數據資料集的對稱性Ω上所花的時間，可能就能做完分群了。\n'
                    '\n'
                    '(可能是這個原因 做數據科學的人比較少在談Ω)\n'
                    '\n'
                    '儘管如此，我還是覺得分析數據資料集的整體對稱性的這個課題\n'
                    '\n'
                    '在學術研究甚至是應用層面上都是非常有意義的!\n'
                    '\n'
                    '\n'
                    'P.S. 似乎目前有看到一些人是用拓樸的方式來描述數據的結構或是對稱性\n'
                    '\n',
 'article_date': 'Thu Feb 18 12:15:02 2021',
 'article_id': 'M.1613621707.A.C0C',
 'article_title': 'Re: [討論] 分團問題!?',
 'ip': '...',
 'message_count': 28,
 'messages': ' 等等，在你說的理想之前，先要考慮一件事情：所謂的分群（=====分隔線===== '
             'Clustering）跟分類（Classification）有關鍵性的差異=====分隔線===== '
             '在你這串開頭第一篇中講了「經常會透過分團(Clique,=====分隔線===== cluster analysis) '
             '來將數據做分類」，我認為這邊用詞要更=====分隔線===== '
             '明確小心些，因為你同時講了分群又講了分類=====分隔線===== '
             '然後講回到分群，基本上是非監督式學習為主，所以事實上分=====分隔線===== '
             '群是沒有唯一標準的，所以「必然」不會有所謂「分群就是在=====分隔線===== '
             '找xxx」中的唯一xxx存在=====分隔線===== Spectral Biclustering？=====分隔線===== '
             '一般初接觸Clustering，往往會看到「分群是讓同一群的點盡=====分隔線===== '
             '量相似，而不同群的點盡量不相似」之類的粗略入門介紹，但=====分隔線===== '
             '光是這句話就未必是絕對的=====分隔線===== '
             '所以原Po你所提到的方向，只能說可以建立出「一種分群方式=====分隔線===== '
             '」，而無法說「分群就是這個」=====分隔線===== '
             '像k-means跟DBSCAN找出的分群天差地遠，但不能說誰比較對=====分隔線===== '
             '，所謂的對稱性也沒有必然存在，分群是可以在有向圖的點上=====分隔線===== 進行的=====分隔線===== '
             'Clustering的主要目標本來也就不是要找出肯定的結論，反而=====分隔線===== '
             '是偏重在分割之後能幫助到後續分析就是有益的分群了=====分隔線===== '
             '你對資料取的features決定分群的好壞=====分隔線===== features反映什麼特性 '
             '就分出啥=====分隔線===== '
             'https://scikit-learn.org/stable/modules/biclustering.h=====分隔線===== '
             'tml=====分隔線===== 有些分群方法有自己的特點，並不是可以用同樣方法加速的=====分隔線===== '
             '你可能沒有考慮到有很多分群演算法根本跟對角化一丁點關係=====分隔線===== '
             '都沒有，甚至連同一群的instances要彼此相似這種限制都不=====分隔線===== 存在=====分隔線===== '
             '所謂分群的好壞也沒有絕對性，是依據後續應用而定',
 'url': 'https://www.ptt.cc/bbs/DataScience/M.1613621707.A.C0C.html'}
2021-03-03 22:41:36 [scrapy.core.engine] INFO: Closing spider (finished)
2021-03-03 22:41:36 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: ./output/ptt.json
2021-03-03 22:41:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3371,
 'downloader/request_count': 7,
 'downloader/request_method_count/GET': 7,
 'downloader/response_bytes': 21102,
 'downloader/response_count': 7,
 'downloader/response_status_count/200': 6,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.591631,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 3, 3, 14, 41, 36, 531035),
 'item_scraped_count': 5,
 'log_count/DEBUG': 18,
 'log_count/INFO': 11,
 'log_count/WARNING': 3,
 'request_depth_max': 1,
 'response_received_count': 7,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2021, 3, 3, 14, 41, 34, 939404)}
2021-03-03 22:41:36 [scrapy.core.engine] INFO: Spider closed (finished)
